
# 최근 3일간 뉴스 데이터만

# 현재 날짜 구하기
current_date = datetime.now()

# 최근 3일 전 날짜 구하기
start_date = current_date - timedelta(days=3)

# 날짜 포맷팅
current_date_str = current_date.strftime("%Y.%m.%d")
start_date_str = start_date.strftime("%Y.%m.%d")


# https://news.naver.com/main/list.naver?mode=LS2D&mid=shm&sid2=230&sid1=105&date=20240118


# 각 기사들의 데이터를 종류별로 나눠담을 리스트를 생성합니다. (추후 DataFrame으로 모을 예정)
titles = []
dates = []
articles = []
article_urls = []
press_companies = []
max_page = 5

# 주어진 일자를 쿼리에 맞는 형태로 변경해줍니다.
start_date = start_date_str.replace(".", "")
# 지정한 기간 내 원하는 페이지 수만큼의 기사를 크롤링합니다.
current_call = 1
last_call = (max_page - 1) * 10 + 1 # max_page이 5일 경우 41에 해당 

while current_call <= last_call:
    
    print('\n{}번째 기사글부터 크롤링을 시작합니다.'.format(current_call))

    # 각 기사들의 데이터를 종류별로 나눠담을 리스트를 생성합니다. (추후 DataFrame으로 모을 예정)
    titles = []
    dates = []
    articles = []
    article_urls = []
    press_companies = []



    # 주어진 일자를 쿼리에 맞는 형태로 변경해줍니다.
    start_date = start_date_str.replace(".", "")



    url = 'https://news.naver.com/main/list.naver?mode=LS2D&mid=shm&sid1=105&sid2=230&date='+start_date


    web = requests.get(url).content
    source = BeautifulSoup(web, 'html.parser')

    urls_list = []

    for urls in source.find_all('a', {'class' : "nclicks(itn.2ndcont)"}):
        if urls.attrs["href"].startswith("https://n.news.naver.com"):
            urls_list.append(urls.attrs["href"])
    for url in urls_list:
        headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36'}
        web_news = requests.get(url, headers=headers).content
        source_news = BeautifulSoup(web_news, 'html.parser')

        title = source_news.find('h2', {'class' : 'media_end_head_headline'}).get_text()
        date = source_news.find('span', {'class' : 'media_end_head_info_datestamp_time'}).get_text()
        
        article = source_news.find('div', {'id' : 'newsct_article'}).get_text()
        article = article.replace("\n", "")
        article = article.replace("// flash 오류를 우회하기 위한 함수 추가function _flash_removeCallback() {}", "")
        article = article.replace("동영상 뉴스       ", "")
        article = article.replace("동영상 뉴스", "")
        article = article.strip()

        press_company = source_news.find('em', {'class':'media_end_linked_more_point'}).get_text()
            
        titles.append(title)
        print('Processing article : {}'.format(title))

        dates.append(date)
        articles.append(article)
        press_companies.append(press_company)
        article_urls.append(url)
            
    # 대량의 데이터를 대상으로 크롤링을 할 때에는 요청 사이에 쉬어주는 타이밍을 넣는 것이 좋습니다.
    time.sleep(5)
    current_call += 10